<h1 align="center">Customed scripts for training llama</h1>
<p align="center">
  <a href="https://github.com/Qznan/train_llama">
    <img src="https://img.shields.io/github/stars/Qznan/train_llama.svg?colorA=orange&colorB=orange&logo=github" alt="GitHub stars">
  </a>
  <a href="https://github.com/Qznan/train_llama/issues">
        <img src="https://img.shields.io/github/issues/Qznan/train_llama.svg"
             alt="GitHub issues">
  </a>
  <a href="https://github.com/Qznan/train_llama/">
        <img src="https://img.shields.io/github/last-commit/Qznan/train_llama.svg">
  </a>
   <a href="https://github.com/Qznan/train_llama/blob/main/LICENSE">
        <img src="https://img.shields.io/github/license/Qznan/train_llama.svg">
  </a>
  
</p>

### Newsç½®é¡¶
- ğŸ”¥ æ›´æ–°vllméƒ¨ç½²è„šæœ¬ï¼ˆæ”¹è¿›ï¼šä¸€æ¬¡éƒ¨ç½²langchainå’Œopenaiå‡å¯ä½¿ç”¨ï¼‰

### èƒŒæ™¯
ç”±äºé¡¹ç›®éœ€è¦ï¼Œæ•´ç†å‡ºllamaæ¨¡å‹çš„å¾®è°ƒä»£ç ï¼Œå¹¶å‘å¸ƒä»¥å¤‡ä»½ã€‚ä»£ç å‚è€ƒæ¥æºæ˜¯[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)ï¼Œä¸»è¦æ˜¯å°†ï¼šå¾®è°ƒæ•°æ®tokenizerä¸ºarrowæ•°æ®çš„é€»è¾‘éƒ¨åˆ†ç‹¬ç«‹å‡ºæ¥äº†ï¼Œè§£ç»‘æ•°æ®å‡†å¤‡ä¸å¾®è°ƒæ­¥éª¤ã€‚ä¿®æ”¹äº†éƒ¨åˆ†ä»£ç ã€‚å¢åŠ äº†ä¸€äº›æ³¨é‡Š

ç›®å‰åªæ•´ç†loraå¾®è°ƒéƒ¨åˆ†ã€‚å…¨å‚å¾®è°ƒåªéœ€å†ä¿®æ”¹ä¸€ç‚¹é€»è¾‘è€Œå·²ï¼Œå¾…åç»­æ›´æ–°ã€‚

### ä½¿ç”¨è¯´æ˜å¦‚ä¸‹
```
é¦–å…ˆæ ¹æ®requirementsä¸‹è½½éœ€è¦çš„åŒ…

loraå¾®è°ƒè¯´æ˜
ä¸»è¦æ­¥éª¤ä¸ºï¼š
1ã€è¯»å–å¾®è°ƒæ•°æ®ç”Ÿæˆtokenzieråçš„æ•°æ®ï¼Œä¿å­˜ä¸ºarrowæ–‡ä»¶æ ¼å¼ï¼ˆè¿è¡Œpython pre_tokenizer_inst.pyï¼‰
2ã€ä¿®æ”¹01_run_sft.shä¸­è¿è¡Œè„šæœ¬ä¸­çš„dataset_dirå‚æ•°ï¼ŒæŒ‡å®šä¸ºä¸Šä¸€æ­¥è¾“å‡ºçš„arrowæ–‡ä»¶ç›®å½•ï¼Œè®¾ç½®å…¶ä½™å‚æ•°è¿è¡Œè¿›è¡Œæ¨¡å‹è®­ç»ƒè®­ç»ƒï¼ˆè¿è¡Œ01_run_sft.shï¼‰
3ã€å°†è®­ç»ƒå¾—åˆ°çš„pt_lora_modelè·ŸåŸæ¨¡å‹è¿›è¡Œå‚æ•°åˆå¹¶ï¼Œå¾—åˆ°å®Œæ•´æ¨¡å‹ï¼ˆè¿è¡Œpython merge_llama2_with_chinese_lora_low_mem.pyï¼‰
4ã€å¯å°†å®Œæ•´æ¨¡å‹è·¯å¾„å†™å…¥åˆ°vllmå¯åŠ¨è„šæœ¬è¿›è¡Œéƒ¨ç½²ã€‚

è¯¦ç»†æ­¥éª¤ï¼š
1ã€è¯¦æƒ…æŸ¥çœ‹pre_tokenizer_inst.pyä¸­çš„__main__æ–¹æ³•ï¼Œ
    1.filesä¸­æŒ‡å®šå¾®è°ƒæ•°æ®ï¼Œè¿™é‡Œç»™å‡ºçš„ç¤ºä¾‹æ•°æ®æ˜¯instr_data/cj_instr.jsonã€‚ä¸»è¦åŒ…æ‹¬instructionã€inputå’Œoutputå­—æ®µã€‚
    2.gen_arrow(files, "arrow_data1219", 'tokenizer_chinese_llama', cache_dir=f'{root_path}instr_data/1219/cache')  # ä½¿ç”¨åŸchinese_llamaè¯è¡¨
        tokenizerä¸»è¦å‡½æ•°ï¼Œå‚æ•°ç”±files,å¤„ç†å®Œçš„æ•°æ®ä¿å­˜ç›®å½•åarrow_data1219,æ‰€é‡‡ç”¨çš„åˆ†è¯å™¨tokenizer_chinese_llama,å¤„ç†ä¸­é—´è¿‡ç¨‹æ•°æ®çš„ä¿å­˜è·¯å¾„instr_data/1219/cacheï¼Œ
        æœ€ç»ˆä¼šç”Ÿæˆinstr_data/1219/arrow_data1219æ–‡ä»¶ç›®å½•

2ã€è¿è¡Œ01_run_sftè„šæœ¬
    1ã€01_run_sft.shä¸­æŒ‡å®šdataset_dirå‚æ•°ä¸ºä¸Šä¸€æ­¥çš„/instr_data/1219/arrow_data1219ç›®å½•ã€‚å¹¶ä¿æŒchinese_tokenizer_pathå‚æ•°ä¸ä¸Šä¸€æ­¥æ‰€é‡‡ç”¨çš„çš„åˆ†è¯å™¨ç›¸åŒã€‚
    2ã€å…¶ä½™å¸¸è§ä¿®æ”¹åŒ…æ‹¬ä½¿ç”¨å•æœºå‡ å¡çš„GPUã€‚åœ¨CUDA_VISIBLE_DEVICES=0,1,2ä¸­è®¾ç½®ï¼Œè¿™ä¸ªä¾‹å­ä½¿ç”¨3å¡ï¼Œæ•…nproc-per-nodeä¹Ÿè¦ä¸€å¹¶ä¿®æ”¹ä¸º3
    3ã€è®¾ç½®æ¨¡å‹ä¿å­˜è¾“å‡ºè·¯å¾„output_dir


3ã€è®­ç»ƒå®Œåè¿è¡Œpython merge_llama2_with_chinese_lora_low_mem.py
    python merge_llama2_with_chinese_lora_low_mem.py \
    --base_model path/to/llama2-hf-model \  # åŸºåº§æ¨¡å‹è·¯å¾„
    --lora_model path/to/chinese-llama2-or-alpaca2-lora \  # ä¸Šä¸€æ­¥æ¨¡å‹æ¨¡å‹ä¿å­˜è·¯å¾„ä¸­çš„pt_lora_modelç›®å½•
    --output_type [huggingface|pth|] \  # ä¸€èˆ¬è®¾ç½®ä¸ºhuggingfaceæ ¼å¼
    --output_dir path/to/output-dir # åˆå¹¶åçš„æ–°æ¨¡å‹ä¿å­˜è·¯å¾„
```

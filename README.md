<h1 align="center">Customed scripts for training llama</h1>
<p align="center">
  <a href="https://github.com/Qznan/train_llama">
    <img src="https://img.shields.io/github/stars/Qznan/train_llama.svg?colorA=orange&colorB=orange&logo=github" alt="GitHub stars">
  </a>
  <a href="https://github.com/Qznan/train_llama/issues">
        <img src="https://img.shields.io/github/issues/Qznan/train_llama.svg"
             alt="GitHub issues">
  </a>
  <a href="https://github.com/Qznan/train_llama/">
        <img src="https://img.shields.io/github/last-commit/Qznan/train_llama.svg">
  </a>
   <a href="https://github.com/Qznan/train_llama/blob/main/LICENSE">
        <img src="https://img.shields.io/github/license/Qznan/train_llama.svg">
  </a>
  
</p>

### Newsç½®é¡¶
- ğŸ”¥ æŒ‡ä»¤å¾®è°ƒæ ·æœ¬é•¿åº¦ç²¾ç»†åŒ–å¤„ç†ã€‚
- ğŸ”¥ æ•°æ®å¤„ç†ä»£ç æ”¯æŒæŒ‰æ¯”ä¾‹é‡‡æ ·åæ•´åˆï¼Œæ”¯æŒç»Ÿè®¡æ•°æ®tokensç­‰ä¿¡æ¯ã€‚arrowæ–‡ä»¶å¤§å°ä¼˜åŒ–
- ğŸ”¥ æ”¯æŒé¢„è®­ç»ƒåŠæŒ‡ä»¤å¾®è°ƒï¼Œæ”¯æŒå…¨é‡å‚æ•°åŠlora
- ğŸ”¥ ä¿®æ”¹vllméƒ¨ç½²è„šæœ¬ï¼Œæ•´åˆgenerateæ¥å£(ä¾›langchainä½¿ç”¨)å’Œopenaiæ¥å£ã€‚æä¾›åˆ¤åˆ«å¼è·å¾—ä¸‹ä¸€tokenæ¦‚ç‡çš„è°ƒç”¨æ–¹å¼

### èƒŒæ™¯
ä¸ªäººé¡¹ç›®ä¸­ä¿®æ”¹æ•´ç†çš„llamaæ¨¡å‹çš„é¢„è®­ç»ƒä»¥åŠå¾®è°ƒä»£ç ï¼Œä»£ç å‚è€ƒæ¥æºæ˜¯[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)ï¼Œä¸»è¦æ˜¯å°†ï¼štokenizing training data and save as arrowçš„è¿™éƒ¨åˆ†é€»è¾‘ç‹¬ç«‹å‡ºæ¥ï¼Œä»¥è§£ç»‘æ•°æ®å‡†å¤‡å’Œè®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚ä¼˜åŒ–å’Œéƒ¨åˆ†ä»£ç ï¼Œå¢åŠ æ³¨é‡Šã€‚

### ä½¿ç”¨è¯´æ˜å¦‚ä¸‹
```
é¦–å…ˆæ ¹æ®requirementsä¸‹è½½éœ€è¦çš„åŒ…

loraå¾®è°ƒè¯´æ˜
ä¸»è¦æ­¥éª¤ä¸ºï¼š
1ã€è¯»å–å¾®è°ƒæ•°æ®ç”Ÿæˆtokenzieråçš„æ•°æ®ï¼Œä¿å­˜ä¸ºarrowæ–‡ä»¶æ ¼å¼ï¼ˆè¿è¡Œpython pre_tokenizer_inst.pyï¼‰
2ã€ä¿®æ”¹01_run_sft.shä¸­è¿è¡Œè„šæœ¬ä¸­çš„dataset_dirå‚æ•°ï¼ŒæŒ‡å®šä¸ºä¸Šä¸€æ­¥è¾“å‡ºçš„arrowæ–‡ä»¶ç›®å½•ï¼Œè®¾ç½®å…¶ä½™å‚æ•°è¿è¡Œè¿›è¡Œæ¨¡å‹è®­ç»ƒè®­ç»ƒï¼ˆè¿è¡Œ01_run_sft.shï¼‰
3ã€å°†è®­ç»ƒå¾—åˆ°çš„pt_lora_modelè·ŸåŸæ¨¡å‹è¿›è¡Œå‚æ•°åˆå¹¶ï¼Œå¾—åˆ°å®Œæ•´æ¨¡å‹ï¼ˆè¿è¡Œpython merge_llama2_with_chinese_lora_low_mem.pyï¼‰
4ã€å¯å°†å®Œæ•´æ¨¡å‹è·¯å¾„å†™å…¥åˆ°vllmå¯åŠ¨è„šæœ¬è¿›è¡Œéƒ¨ç½²ã€‚

è¯¦ç»†æ­¥éª¤ï¼š
1ã€è¯¦æƒ…æŸ¥çœ‹pre_tokenizer_inst.pyä¸­çš„__main__æ–¹æ³•ï¼Œ
    1.filesä¸­æŒ‡å®šå¾®è°ƒæ•°æ®ï¼Œè¿™é‡Œç»™å‡ºçš„ç¤ºä¾‹æ•°æ®æ˜¯instr_data/cj_instr.jsonã€‚ä¸»è¦åŒ…æ‹¬instructionã€inputå’Œoutputå­—æ®µã€‚
    2.gen_arrow(files, "arrow_data1219", 'tokenizer_chinese_llama', cache_dir=f'{root_path}instr_data/1219/cache')  # ä½¿ç”¨åŸchinese_llamaè¯è¡¨
        tokenizerä¸»è¦å‡½æ•°ï¼Œå‚æ•°ç”±files,å¤„ç†å®Œçš„æ•°æ®ä¿å­˜ç›®å½•åarrow_data1219,æ‰€é‡‡ç”¨çš„åˆ†è¯å™¨tokenizer_chinese_llama,å¤„ç†ä¸­é—´è¿‡ç¨‹æ•°æ®çš„ä¿å­˜è·¯å¾„instr_data/1219/cacheï¼Œ
        æœ€ç»ˆä¼šç”Ÿæˆinstr_data/1219/arrow_data1219æ–‡ä»¶ç›®å½•

2ã€è¿è¡Œ01_run_sftè„šæœ¬
    1ã€01_run_sft.shä¸­æŒ‡å®šdataset_dirå‚æ•°ä¸ºä¸Šä¸€æ­¥çš„/instr_data/1219/arrow_data1219ç›®å½•ã€‚å¹¶ä¿æŒchinese_tokenizer_pathå‚æ•°ä¸ä¸Šä¸€æ­¥æ‰€é‡‡ç”¨çš„çš„åˆ†è¯å™¨ç›¸åŒã€‚
    2ã€å…¶ä½™å¸¸è§ä¿®æ”¹åŒ…æ‹¬ä½¿ç”¨å•æœºå‡ å¡çš„GPUã€‚åœ¨CUDA_VISIBLE_DEVICES=0,1,2ä¸­è®¾ç½®ï¼Œè¿™ä¸ªä¾‹å­ä½¿ç”¨3å¡ï¼Œæ•…nproc-per-nodeä¹Ÿè¦ä¸€å¹¶ä¿®æ”¹ä¸º3
    3ã€è®¾ç½®æ¨¡å‹ä¿å­˜è¾“å‡ºè·¯å¾„output_dir


3ã€è®­ç»ƒå®Œåè¿è¡Œpython merge_llama2_with_chinese_lora_low_mem.py
    python merge_llama2_with_chinese_lora_low_mem.py \
    --base_model path/to/llama2-hf-model \  # åŸºåº§æ¨¡å‹è·¯å¾„
    --lora_model path/to/chinese-llama2-or-alpaca2-lora \  # ä¸Šä¸€æ­¥æ¨¡å‹æ¨¡å‹ä¿å­˜è·¯å¾„ä¸­çš„pt_lora_modelç›®å½•
    --output_type [huggingface|pth|] \  # ä¸€èˆ¬è®¾ç½®ä¸ºhuggingfaceæ ¼å¼
    --output_dir path/to/output-dir # åˆå¹¶åçš„æ–°æ¨¡å‹ä¿å­˜è·¯å¾„
```
